{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebab2298",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "`Data preprocessing` is the essential stage in machine learning that ensures raw data is transformed into a clean, structured, and consistent format suitable for training reliable models. Since real-world data is often noisy, incomplete, and inconsistent, the preprocessing stage plays a key role in enhancing model accuracy, generalization, and interpretability.\n",
    "\n",
    "The modern data preprocessing pipeline typically includes the following **11 key steps**:\n",
    "\n",
    "- **Data Collection:** Acquiring relevant, high-quality datasets from appropriate sources to represent the problem.\n",
    "\n",
    "- **Data Cleaning:** Detecting and correcting errors, inconsistencies, or duplicates; handling noise, irrelevant data, and anomalies to ensure data integrity.\n",
    "\n",
    "- **Handling Missing Values:** Managing gaps in the dataset using techniques such as mean, median, or mode imputation, or more advanced methods like KNN or regression-based imputation.\n",
    "\n",
    "- **Data Integration:** Merging data from multiple sources or formats (databases, APIs, flat files) into a unified dataset while ensuring consistency and removing redundancy.\n",
    "\n",
    "- **Data Transformation:** Normalizing, standardizing, encoding categorical variables, and applying domain-specific conversions to make all features numerically consistent for modeling.\n",
    "\n",
    "- **Feature Engineering:** Creating new or derived features (e.g., ratios or interaction terms) that reveal hidden relationships or improve predictive performance.\n",
    "\n",
    "- **Outlier Detection and Treatment:** Identifying and mitigating the influence of extreme values using methods such as IQR capping or percentile winsorization.\n",
    "\n",
    "- **Skewness Reduction / Log Transformation:** Reducing skew in feature distributions using transformations (e.g., log, Box-Cox) to stabilize variance and improve model convergence.\n",
    "\n",
    "- **Multicollinearity Detection (Variance Inflation Factor - VIF):** Measuring correlation among features to remove redundant variables that can distort model interpretation.\n",
    "\n",
    "- **Handling Imbalanced Data:** Applying resampling methods like SMOTE or undersampling to balance the target variable distribution and improve classification fairness.\n",
    "\n",
    "- **Data Reduction and Preprocessing Pipelines:** Using dimensionality reduction (e.g., PCA or feature selection) and automated `Pipeline` workflows for streamlined preprocessing, ensuring reproducibility and efficient deployment.\n",
    "\n",
    "These steps collectively ensure that the dataset is high-quality, balanced, and feature-ready, ultimately enhancing both the performance and reliability of any machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fd9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppressing Warnings\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Ignore RuntimeWarnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf3bbc",
   "metadata": {},
   "source": [
    "## Implementing The Data Preprocessing Concepts on a Synthetic Dataset\n",
    "- Let's create a synthetic dataset and walk through all the data preprocessing steps with code and explanations at each step. This will consolidate your understanding with practical implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0cc37d",
   "metadata": {},
   "source": [
    "**Step 1**: Synthetic Dataset Creation\n",
    "- We generate a dataset with numerical and categorical variables, purposely inserting missing values and outliers to simulate real-world messy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa7cd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Annual Income (k$)  Membership Years  Credit Score  Account Balance  \\\n",
      "0   56           35.903305                 5    573.764392     22582.586422   \n",
      "1   69           63.051955                 4    639.092497     24864.982727   \n",
      "2   46           48.654739                15    652.650334     90612.652841   \n",
      "3   32           38.666194                 4    690.920084     25911.764220   \n",
      "4   60           50.301407                10    650.266215     22408.918539   \n",
      "\n",
      "   Gender   City Purchased  Income Proxy  \n",
      "0    Male  CityC        No     20.049777  \n",
      "1    Male  CityB        No      5.341909  \n",
      "2  Female  CityC        No     39.905317  \n",
      "3  Female  CityC        No      3.974243  \n",
      "4  Female  CityB       Yes     25.870999  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "size = 1000\n",
    "\n",
    "data = {\n",
    "    'Age': np.random.randint(18, 70, size),\n",
    "    'Annual Income (k$)': np.random.normal(60, 15, size),\n",
    "    'Membership Years': np.random.randint(1, 20, size),\n",
    "    'Credit Score': np.random.normal(650, 70, size),\n",
    "    'Account Balance': np.random.normal(20000, 5000, size),\n",
    "    'Gender': np.random.choice(['Male', 'Female'], size),\n",
    "    'City': np.random.choice(['CityA', 'CityB', 'CityC'], size),\n",
    "    'Purchased': np.random.choice(['Yes', 'No'], size, p=[0.3, 0.7])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce missing values intentionally\n",
    "missing_indices = np.random.choice(df.index, 50, replace=False)\n",
    "df.loc[missing_indices, 'Annual Income (k$)'] = np.nan\n",
    "\n",
    "# Introduce outliers intentionally\n",
    "outliers_idx = np.random.choice(df.index, 10, replace=False)\n",
    "df.loc[outliers_idx, 'Account Balance'] *= 5\n",
    "\n",
    "# Add correlated feature to create multicollinearity\n",
    "noise = np.random.normal(0, 5, size)\n",
    "df['Income Proxy'] = df['Membership Years'] * 3 + noise\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6832242",
   "metadata": {},
   "source": [
    "**Explanation**: We simulate numeric and categorical features, add missing entries, outliers, and a correlated column to create realistic preprocessing challenges.\n",
    "\n",
    "**Step 2**: Data Cleaning - Handling Missing Values\n",
    "- Detect missing values and impute with the column mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074d42a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation:\n",
      " Age                    0\n",
      "Annual Income (k$)    50\n",
      "Membership Years       0\n",
      "Credit Score           0\n",
      "Account Balance        0\n",
      "Gender                 0\n",
      "City                   0\n",
      "Purchased              0\n",
      "Income Proxy           0\n",
      "dtype: int64\n",
      "Missing values after imputation:\n",
      " Age                   0\n",
      "Annual Income (k$)    0\n",
      "Membership Years      0\n",
      "Credit Score          0\n",
      "Account Balance       0\n",
      "Gender                0\n",
      "City                  0\n",
      "Purchased             0\n",
      "Income Proxy          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"Missing values before imputation:\\n\", df.isnull().sum())\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df['Annual Income (k$)'] = imputer.fit_transform(df[['Annual Income (k$)']])\n",
    "\n",
    "print(\"Missing values after imputation:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e4252",
   "metadata": {},
   "source": [
    "**Explanation**: Handling missing values prevents issues during modeling; mean imputation fills gaps using average income.\n",
    "\n",
    "**Step 3**: Data Integration\n",
    "- This synthetic dataset is already integrated, so no extra action is required here.\n",
    "\n",
    "**Step 4**: Data Transformation - Encoding and Scaling\n",
    "- Build pipelines to encode categorical features and scale numeric ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4084c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transformation done.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_cols = ['Age', 'Annual Income (k$)', 'Membership Years', 'Credit Score', 'Account Balance', 'Income Proxy']\n",
    "cat_cols = ['Gender', 'City']\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "X = df.drop(columns=['Purchased'])\n",
    "y = df['Purchased']\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"Data transformation done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2277d",
   "metadata": {},
   "source": [
    "**Explanation**: Scaling numeric features helps algorithms converge; encoding transforms categories to numeric form.\n",
    "\n",
    "**Step 5**: Feature Engineering\n",
    "- Create a derived feature representing income per year of membership (handling divide-by-zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbcb27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Annual Income (k$)  Membership Years  Income_per_Year\n",
      "0           35.903305                 5         7.180661\n",
      "1           63.051955                 4        15.762989\n",
      "2           48.654739                15         3.243649\n",
      "3           38.666194                 4         9.666549\n",
      "4           50.301407                10         5.030141\n"
     ]
    }
   ],
   "source": [
    "df['Income_per_Year'] = df['Annual Income (k$)'] / df['Membership Years'].replace(0, 1)\n",
    "print(df[['Annual Income (k$)', 'Membership Years', 'Income_per_Year']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b733c86",
   "metadata": {},
   "source": [
    "**Explanation**: New meaningful features can improve model insight and performance.\n",
    "\n",
    "**Step 6**: Outlier Detection and Treatment using Interquartile Range (IQR)\n",
    "Cap extreme Account Balance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14659d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers capped using IQR method.\n"
     ]
    }
   ],
   "source": [
    "Q1 = df['Account Balance'].quantile(0.25)\n",
    "Q3 = df['Account Balance'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df['Account Balance'] = np.where(df['Account Balance'] > upper_bound, upper_bound, \n",
    "                                 np.where(df['Account Balance'] < lower_bound, lower_bound, df['Account Balance']))\n",
    "print(\"Outliers capped using IQR method.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ab157",
   "metadata": {},
   "source": [
    "**Explanation**: Reducing outlier impact ensures model robustness.\n",
    "\n",
    "**Step 7**: Skewness Reduction\n",
    "- Apply log transformation to skewed numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff64647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log transformation applied to reduce skewness.\n"
     ]
    }
   ],
   "source": [
    "skewed_cols = ['Annual Income (k$)', 'Account Balance']\n",
    "for col in skewed_cols:\n",
    "    df[col] = np.log1p(df[col].clip(lower=0))  # avoid log of negative values\n",
    "\n",
    "print(\"Log transformation applied to reduce skewness.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d2cab",
   "metadata": {},
   "source": [
    "**Explanation**: Normalizing distributions improves model stability and predictions.\n",
    "\n",
    "**Step 8**: Multicollinearity Detection with Variance Inflation Factor (VIF)\n",
    "- Identify features highly correlated with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a388bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Feature         VIF\n",
      "0                 Age    9.551975\n",
      "1  Annual Income (k$)  232.702907\n",
      "2    Membership Years   52.026000\n",
      "3        Credit Score   88.325438\n",
      "4     Account Balance  269.729841\n",
      "5        Income Proxy   44.620573\n",
      "6     Income_per_Year    3.331559\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X_vif = df[num_cols + ['Income_per_Year']].copy()\n",
    "\n",
    "# Replace infinite values and impute missing before VIF\n",
    "X_vif.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_vif.fillna(X_vif.mean(), inplace=True)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['Feature'] = X_vif.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57c5b7",
   "metadata": {},
   "source": [
    "**Explanation**: Detecting multicollinearity guides feature selection and prevents unreliable coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58179e8f",
   "metadata": {},
   "source": [
    "**Step 9**:Handling Imbalanced Data with Random Oversampling\n",
    "- Random oversampling is a simple technique that balances the dataset by duplicating existing samples from the minority class until the class sizes are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2038bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "Purchased\n",
      "0    685\n",
      "1    315\n",
      "Name: count, dtype: int64\n",
      "Balanced class distribution after Random Oversampling:\n",
      "0    685\n",
      "1    685\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Map target to binary labels\n",
    "y_binary = df['Purchased'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Apply preprocessing pipeline on features\n",
    "X_transformed = preprocessor.fit_transform(df.drop(columns=['Purchased']))\n",
    "\n",
    "# Combine features and target into one DataFrame for resampling\n",
    "df_balancer = pd.concat([pd.DataFrame(X_transformed), y_binary.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_balancer[df_balancer[y_binary.name] == 0]\n",
    "df_minority = df_balancer[df_balancer[y_binary.name] == 1]\n",
    "\n",
    "# Upsample minority class by random repetition\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,\n",
    "                                 n_samples=len(df_majority),\n",
    "                                 random_state=42)\n",
    "\n",
    "# Combine majority and upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Extract features and target\n",
    "X_res = df_upsampled.drop(columns=[y_binary.name]).values\n",
    "y_res = df_upsampled[y_binary.name].values\n",
    "\n",
    "# Display distributions\n",
    "print(f\"Original class distribution:\\n{y_binary.value_counts()}\")\n",
    "print(f\"Balanced class distribution after Random Oversampling:\\n{pd.Series(y_res).value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db86e0b",
   "metadata": {},
   "source": [
    "**Explanation**: This approach increases the minority class size by replication rather than synthetic generation, helping classifiers learn better decision boundaries without introducing artificial variation.\n",
    "\n",
    "**Step 10**: Data Splitting\n",
    "- Split the balanced dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b610e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: (1096, 9), Testing size: (274, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "print(f\"Training size: {X_train.shape}, Testing size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32d7fe",
   "metadata": {},
   "source": [
    "**Step 11**: Pipeline Automation\n",
    "\n",
    "- Each transformation is encapsulated in pipelines ensuring consistent application and preventing data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e450fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All numerical and categorical preprocessing steps completed in pipeline.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "num_cols = ['Age', 'Annual Income (k$)', 'Membership Years', 'Credit Score', 'Account Balance', 'Income Proxy', 'Income_per_Year']\n",
    "cat_cols = ['Gender', 'City']\n",
    "\n",
    "# Function to clip outliers based on IQR\n",
    "def cap_outliers(X):\n",
    "    # Assume X is a 2D numpy array for numerical columns\n",
    "    X = X.copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        column = X[:, i]\n",
    "        Q1 = np.percentile(column, 25)\n",
    "        Q3 = np.percentile(column, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        column = np.clip(column, lower, upper)\n",
    "        X[:, i] = column\n",
    "    return X\n",
    "\n",
    "# Function to apply log1p transformation to reduce skewness\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "\n",
    "# Numerical pipeline with all steps: missing value imputation, outlier capping, log transform and scaling\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),     # Handle missing values\n",
    "    ('outlier_cap', FunctionTransformer(cap_outliers, validate=False)),  # Clip outliers\n",
    "    ('log_transform', log_transformer),               # Reduce skewness\n",
    "    ('scaler', StandardScaler())                      # Scale features\n",
    "])\n",
    "\n",
    "# Categorical pipeline: missing value imputation + one hot encoding\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "])\n",
    "\n",
    "# Full preprocessing pipeline\n",
    "preprocessing_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform with pipeline\n",
    "X_processed = preprocessing_pipeline.fit_transform(df.drop(columns=['Purchased']))\n",
    "\n",
    "print(\"All numerical and categorical preprocessing steps completed in pipeline.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b2b22",
   "metadata": {},
   "source": [
    "- #### Observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272771af",
   "metadata": {},
   "source": [
    "1. Data Creation - synthetic dataset is created with purposely insrted missing values and outliers.\n",
    "2. Data Cleaning - handling missing values using SimpleImputer. Before imputation annual income column had 50 missing values. Imputation fills gaps using average income.\n",
    "3. Data Transformation - Encoding and Scaling using OneHotEncoding and StandardScaler. StandardScaler helps achieve standardization and  OneHotEncoding converts categorical data into numerical form.\n",
    "4. Feature Engineering - creating new feature 'Income_per_year'.\n",
    "5. Outlier Detection and Treatment using Interquartile Range (IQR), because outliers can affect model accuracy and mislead results.\n",
    "6. Skewness Reduction - skewness measures how asymmetric a data distribution is around its mean. Reducing skewness helps achieve normalization that improves model accuracy and stability.\n",
    "7.  Multicollinearity Detection - Identify features highly correlated with others.\n",
    "8. Handling Imbalanced Data with Random Oversampling.\n",
    "9. Data Splitting - Split the balanced dataset into training and testing sets.\n",
    "10. Pipeline Automation - the process of automating a sequence of steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
