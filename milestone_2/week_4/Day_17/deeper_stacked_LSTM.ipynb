{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7871301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Import additional layers and tools for experimentation ---\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout, Input, Layer\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d63ce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata exists: True\n",
      "metadata shape: (152559, 6)\n",
      "   engine_id  cycle  max_cycle_x    RUL  max_cycle_y  max_cycle_new\n",
      "0          1    8.0        321.0  313.0        321.0          321.0\n",
      "1          1    8.0        321.0  313.0        321.0          321.0\n",
      "2          1    9.0        321.0  312.0        321.0          321.0\n",
      "3          1    9.0        321.0  312.0        321.0          321.0\n",
      "4          1    9.0        321.0  312.0        321.0          321.0\n",
      "columns: ['engine_id', 'cycle', 'max_cycle_x', 'RUL', 'max_cycle_y', 'max_cycle_new']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "p_meta = Path(r\"D:/PROGONSAI/milestone_2/week_4/Day_16/sequence_metadata_with_RUL.csv\")\n",
    "print(\"metadata exists:\", p_meta.exists())\n",
    "meta = pd.read_csv(p_meta)\n",
    "print(\"metadata shape:\", meta.shape)\n",
    "print(meta.head())\n",
    "# if there's a column indicating the original sequence length or n_features, print columns:\n",
    "print(\"columns:\", meta.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b8efa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N (windows) = 152559\n",
      "file size (bytes) = 1208267280\n",
      "flat length (elements) = 302066820\n",
      "per_window elements = 1980 (should be 1980)\n",
      "Candidate (window_size, n_features) pairs (first 40):\n",
      "[(1980, 1), (990, 2), (660, 3), (495, 4), (396, 5), (330, 6), (220, 9), (198, 10), (180, 11), (165, 12), (132, 15), (110, 18), (99, 20), (90, 22), (66, 30), (60, 33), (55, 36), (45, 44), (44, 45), (36, 55), (33, 60), (30, 66), (22, 90), (20, 99), (18, 110), (15, 132), (12, 165), (11, 180), (10, 198)]\n",
      "\n",
      "Tried reshape -> (N, 1980, 1) = (152559, 1980, 1)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 1.168420672416687, 0.3459184467792511, -1.1964118480682373, -0.9895810484886169, -0.917426347732544]\n",
      "\n",
      "Tried reshape -> (N, 990, 2) = (152559, 990, 2)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 0.3459184467792511, -0.9895810484886169, -0.9076555967330933, -0.9968631267547607, -0.3593102991580963]\n",
      "\n",
      "Tried reshape -> (N, 660, 3) = (152559, 660, 3)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -1.1964118480682373, -0.9076555967330933, -0.9478903412818909, -0.9407589435577393, 0.34246304631233215]\n",
      "\n",
      "Tried reshape -> (N, 495, 4) = (152559, 495, 4)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -0.9895810484886169, -0.9968631267547607, -0.9407589435577393, -0.5010786056518555, -0.356103777885437]\n",
      "\n",
      "Tried reshape -> (N, 396, 5) = (152559, 396, 5)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -0.917426347732544, -0.3593102991580963, 0.34246304631233215, -0.356103777885437, 2.3198935985565186]\n",
      "\n",
      "Tried reshape -> (N, 330, 6) = (152559, 330, 6)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -0.9076555967330933, -0.9407589435577393, -1.037615418434143, -0.10941052436828613, 0.10882361978292465]\n",
      "\n",
      "Tried reshape -> (N, 220, 9) = (152559, 220, 9)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -0.9478903412818909, -1.037615418434143, 1.7686126232147217, 0.15479673445224762, 0.6580288410186768]\n",
      "\n",
      "Tried reshape -> (N, 198, 10) = (152559, 198, 10)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -0.3593102991580963, -0.356103777885437, 0.10882361978292465, 0.082790307700634, 0.11699016392230988]\n",
      "\n",
      "Tried reshape -> (N, 180, 11) = (152559, 180, 11)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -0.8957961201667786, -0.9588758945465088, 2.0091187953948975, 0.29860010743141174, 1.489680290222168]\n",
      "\n",
      "Tried reshape -> (N, 165, 12) = (152559, 165, 12)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -0.9407589435577393, -0.10941052436828613, 0.15479673445224762, 0.7497531771659851, 0.7542818188667297]\n",
      "\n",
      "Tried reshape -> (N, 132, 15) = (152559, 132, 15)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 0.34246304631233215, 0.10882361978292465, 0.6580288410186768, 0.7542818188667297, 1.1157710552215576]\n",
      "\n",
      "Tried reshape -> (N, 110, 18) = (152559, 110, 18)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -1.037615418434143, 0.15479673445224762, -0.07028445601463318, 0.9971663355827332, 0.5935134887695312]\n",
      "\n",
      "Tried reshape -> (N, 99, 20) = (152559, 99, 20)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -0.356103777885437, 0.082790307700634, 0.7542818188667297, 1.1180137395858765, 0.7533332109451294]\n",
      "\n",
      "Tried reshape -> (N, 90, 22) = (152559, 90, 22)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -0.9588758945465088, 0.29860010743141174, -1.0416802167892456, 1.1265451908111572, 0.7740747332572937]\n",
      "\n",
      "Tried reshape -> (N, 66, 30) = (152559, 66, 30)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 0.10882361978292465, 0.7542818188667297, 0.5935134887695312, 0.5685237646102905, -1.037615418434143]\n",
      "\n",
      "Tried reshape -> (N, 60, 33) = (152559, 60, 33)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 2.0091187953948975, -1.0416802167892456, 1.1235525608062744, 1.4998531341552734, 1.4886548519134521]\n",
      "\n",
      "Tried reshape -> (N, 55, 36) = (152559, 55, 36)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 0.15479673445224762, 0.9971663355827332, 0.6987795829772949, -1.011134386062622, 0.7510261535644531]\n",
      "\n",
      "Tried reshape -> (N, 45, 44) = (152559, 45, 44)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 0.29860010743141174, 1.1265451908111572, 1.4998531341552734, 0.22881442308425903, 1.1171358823776245]\n",
      "\n",
      "Tried reshape -> (N, 44, 45) = (152559, 44, 45)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 0.6580288410186768, 0.5935134887695312, -1.3423727750778198, 0.7510261535644531, 0.8554472327232361]\n",
      "\n",
      "Tried reshape -> (N, 36, 55) = (152559, 36, 55)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 1.489680290222168, 0.7740747332572937, 1.4886548519134521, 1.1171358823776245, -0.9970968961715698]\n",
      "\n",
      "Tried reshape -> (N, 33, 60) = (152559, 33, 60)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 0.7542818188667297, 0.5685237646102905, 0.7510261535644531, 0.42605769634246826, -0.4639369249343872]\n",
      "\n",
      "Tried reshape -> (N, 30, 66) = (152559, 30, 66)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -1.0416802167892456, 1.4998531341552734, -1.0415350198745728, 1.4994475841522217, -1.0416016578674316]\n",
      "\n",
      "Tried reshape -> (N, 22, 90) = (152559, 22, 90)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 0.5935134887695312, 0.7510261535644531, -0.9916667342185974, 0.356518030166626, -0.4535693824291229]\n",
      "\n",
      "Tried reshape -> (N, 20, 99) = (152559, 20, 99)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 1.1235525608062744, -1.0415350198745728, 1.3768179416656494, 0.168569415807724, 0.7526780366897583]\n",
      "\n",
      "Tried reshape -> (N, 18, 110) = (152559, 18, 110)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 0.7740747332572937, 1.1171358823776245, -1.0416016578674316, 0.16740268468856812, -1.014475703239441]\n",
      "\n",
      "Tried reshape -> (N, 15, 132) = (152559, 15, 132)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 1.4998531341552734, 1.4994475841522217, 0.168569415807724, 0.4709003269672394, 1.4997864961624146]\n",
      "\n",
      "Tried reshape -> (N, 12, 165) = (152559, 12, 165)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 1.4886548519134521, -1.0416016578674316, 0.7526780366897583, 1.4997864961624146, 1.2496458292007446]\n",
      "\n",
      "Tried reshape -> (N, 11, 180) = (152559, 11, 180)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, 0.7510261535644531, 0.356518030166626, -1.5037627220153809, -0.6562090516090393, 0.42605769634246826]\n",
      "\n",
      "Tried reshape -> (N, 10, 198) = (152559, 10, 198)\n",
      " dtype: float32\n",
      " global min/max/mean: -6.302419662475586 4.867166042327881 0.006761365570127964\n",
      " first window min/max/mean: -3.4811782836914062 2.4503772258758545 0.260800838470459\n",
      " sample of first window, feature 0 (first 6 values): [1.0759191513061523, -1.0415350198745728, 0.168569415807724, -1.0417346954345703, 1.500022530555725, -1.0417649745941162]\n",
      "\n",
      "No save performed. Inspect the printed candidate pairs above and set `pair_to_save` to the right (window_size, n_features).\n"
     ]
    }
   ],
   "source": [
    "# Recovery helper: try factor pairs and show quick sanity checks\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "p = Path(r\"D:/PROGONSAI/milestone_2/week_4/Day_16/rolling_window_sequences_float32.npy\")\n",
    "meta = pd.read_csv(Path(r\"D:/PROGONSAI/milestone_2/week_4/Day_16/sequence_metadata_with_RUL.csv\"))\n",
    "N = meta.shape[0]\n",
    "print(\"N (windows) =\", N)\n",
    "print(\"file size (bytes) =\", p.stat().st_size)\n",
    "\n",
    "# Step 1: load raw float32 flat array\n",
    "arr = np.fromfile(str(p), dtype=np.float32)\n",
    "print(\"flat length (elements) =\", arr.size)\n",
    "if arr.size != (p.stat().st_size // 4):\n",
    "    print(\"Warning: file size not divisible by 4 exactly.\")\n",
    "per_window = arr.size // N\n",
    "print(\"per_window elements =\", per_window, \"(should be 1980)\")\n",
    "\n",
    "# Step 2: list factor pairs of per_window (reasonable feature counts up to 200)\n",
    "pairs = []\n",
    "for f in range(1,201):           # test feature counts 1..200\n",
    "    if per_window % f == 0:\n",
    "        w = per_window // f\n",
    "        if 1 <= w <= 2000:      # reasonable timesteps limit\n",
    "            pairs.append((w, f))\n",
    "print(\"Candidate (window_size, n_features) pairs (first 40):\")\n",
    "print(pairs[:40])\n",
    "\n",
    "# Step 3: try reshaping for each candidate and run sanity checks\n",
    "def sanity_check(X):\n",
    "    # quick checks on values to detect obvious garbage\n",
    "    s = {}\n",
    "    s['shape'] = X.shape\n",
    "    s['dtype'] = X.dtype\n",
    "    # stats on entire array\n",
    "    s['global_min'] = float(np.nanmin(X))\n",
    "    s['global_max'] = float(np.nanmax(X))\n",
    "    s['global_mean'] = float(np.nanmean(X))\n",
    "    # stats on first window (index 0)\n",
    "    first = X[0]\n",
    "    s['first_min'] = float(np.nanmin(first))\n",
    "    s['first_max'] = float(np.nanmax(first))\n",
    "    s['first_mean'] = float(np.nanmean(first))\n",
    "    return s\n",
    "\n",
    "attempts = []\n",
    "for (w,f) in pairs:\n",
    "    expected = N * w * f\n",
    "    if expected != arr.size:\n",
    "        continue\n",
    "    try:\n",
    "        X = arr.reshape((N, w, f))\n",
    "        stats = sanity_check(X)\n",
    "        attempts.append((w,f,stats))\n",
    "        # print summary for first few attempts (more verbose)\n",
    "        print(f\"\\nTried reshape -> (N, {w}, {f}) = {X.shape}\")\n",
    "        print(\" dtype:\", stats['dtype'])\n",
    "        print(\" global min/max/mean:\", stats['global_min'], stats['global_max'], stats['global_mean'])\n",
    "        print(\" first window min/max/mean:\", stats['first_min'], stats['first_max'], stats['first_mean'])\n",
    "        # show first 6 values of first feature series (first window, first feature)\n",
    "        print(\" sample of first window, feature 0 (first 6 values):\", X[0, :6, 0].tolist())\n",
    "    except Exception as e:\n",
    "        print(\"Failed reshape for\", (w,f), \"->\", e)\n",
    "\n",
    "if not attempts:\n",
    "    raise RuntimeError(\"No valid reshape attempts — something unexpected. But arr length/division matched so there should be candidates.\")\n",
    "\n",
    "# Step 4: when you identify the correct pair, save it (uncomment and edit pair_to_save)\n",
    "# Example: pair_to_save = (30, 66)\n",
    "pair_to_save = None   # <-- set this to the correct (window_size, n_features) AFTER you inspect above attempts\n",
    "\n",
    "if pair_to_save is not None:\n",
    "    w,f = pair_to_save\n",
    "    X = arr.reshape((N, w, f))\n",
    "    print(\"Saving recovered array to rolling_window_sequences_float32_recovered.npy and .npz ...\")\n",
    "    np.save(\"rolling_window_sequences_float32_recovered.npy\", X, allow_pickle=False)\n",
    "    np.savez_compressed(\"rolling_window_sequences_recovered.npz\", X=X)\n",
    "    print(\"Saved shapes:\", X.shape)\n",
    "else:\n",
    "    print(\"\\nNo save performed. Inspect the printed candidate pairs above and set `pair_to_save` to the right (window_size, n_features).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4b63b",
   "metadata": {},
   "source": [
    "### 1. Deeper Stacked LSTM Model with Dropout and L2 Regularization\n",
    "\n",
    "A deeper stacked LSTM model consists of multiple LSTM layers placed sequentially, allowing the network to learn complex temporal dependencies and hierarchical features by capturing short-term patterns in lower layers and longer-term ones in higher layers. To improve generalization and reduce overfitting, dropout is applied after each LSTM layer, randomly dropping neurons during training to prevent reliance on any single unit, which is important for deep models with many parameters. Additionally, L2 regularization adds a penalty on the squared magnitude of weights during training, encouraging smaller weights and controlling model complexity, making the combination of stacked LSTM, dropout, and L2 regularization a practical approach to building deep, robust, and expressive models for time-series forecasting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a8d4ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m67,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">100,417</span> (392.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m100,417\u001b[0m (392.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">100,417</span> (392.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m100,417\u001b[0m (392.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model with 2 stacked LSTM layers, dropout and L2 weight regularization\n",
    "def build_stacked_lstm_model(input_shape, lstm_units=64, dropout_rate=0.3, l2_reg=1e-4):\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, activation='tanh', return_sequences=True,\n",
    "             kernel_regularizer=regularizers.l2(l2_reg),\n",
    "             input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units, activation='tanh', return_sequences=False,\n",
    "             kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model_stacked = build_stacked_lstm_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "model_stacked.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "model_stacked.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6721e",
   "metadata": {},
   "source": [
    "### 2. Bidirectional LSTM Model\n",
    "\n",
    "A bidirectional LSTM (BiLSTM) enhances the traditional LSTM by processing input sequences in both forward (past to future) and backward (future to past) directions, enabling the model to access context from both preceding and succeeding time steps. This dual approach provides richer sequence understanding and captures dependencies that unidirectional LSTMs might miss, making BiLSTMs especially valuable in tasks like Remaining Useful Life (RUL) prediction and natural language processing. The outputs from both directions are combined—usually by concatenation or summation—to form a comprehensive representation at each time step. While bidirectional LSTMs improve context awareness and accuracy, they require the entire sequence upfront, leading to increased training time and computational complexity, thus being better suited for offline or batch processing rather than real-time streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db1dfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM layer to capture forward and backward temporal dependencies\n",
    "def build_bidirectional_lstm_model(input_shape, lstm_units=64, dropout_rate=0.3):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=False),\n",
    "                      input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dee442",
   "metadata": {},
   "source": [
    "### 3. Attention Mechanism Layer (Basic Additive Attention)\n",
    "\n",
    "The attention mechanism enables neural networks to dynamically focus on the most relevant parts of input sequences by computing alignment scores between the current state and each sequence element using learned weights and biases. These scores are normalized via softmax to generate attention weights that highlight important time steps, producing a weighted sum that emphasizes critical temporal information instead of treating all inputs equally. This approach overcomes the limitations of fixed-length summaries in standard RNNs or LSTMs by selectively pooling important features, improving model capacity and accuracy in tasks like language translation and time-series forecasting. Implemented as an attention layer over LSTM hidden states, it assigns importance scores that help the model focus on key sequence parts indicative of outcomes like remaining useful life, while also adding interpretability by allowing visualization of which time frames influenced predictions—making attention highly useful in predictive maintenance and RUL estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f12850b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\roji2\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m198\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m67,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ attention (\u001b[38;5;33mAttention\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m74\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,467</span> (263.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,467\u001b[0m (263.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,467</span> (263.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,467\u001b[0m (263.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple Attention Layer Definition\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# Model with attention after LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_lstm_attention_model(input_shape, lstm_units=64, dropout_rate=0.3):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    attention_out = Attention()(lstm_out)\n",
    "    dropout_out = Dropout(dropout_rate)(attention_out)\n",
    "    outputs = Dense(1)(dropout_out)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model_attention = build_lstm_attention_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "model_attention.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "model_attention.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
