{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import additional layers and tools for experimentation ---\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout, Input, Layer\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63ce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (152559, 30, 66)\n",
      "y shape: (152559,)\n"
     ]
    }
   ],
   "source": [
    "# Load rolling window sequences (shape: [num_samples, window_size, num_features])\n",
    "X = np.load(\"C:/Users/win10/Desktop/Project_Oct25/prognosAI-Infosys-intern-project/data/preprocessed/rolling_window_sequences.npy\")      # Feature array\n",
    "\n",
    "# Load metadata that contains engine_id, cycle, RUL, etc.\n",
    "metadata = pd.read_csv(\"C:/Users/win10/Desktop/Project_Oct25/prognosAI-Infosys-intern-project/data/preprocessed/sequence_metadata_with_RUL.csv\")\n",
    "y = metadata[\"RUL\"].values                     # Target RUL array\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4b63b",
   "metadata": {},
   "source": [
    "### 1. Deeper Stacked LSTM Model with Dropout and L2 Regularization\n",
    "\n",
    "A deeper stacked LSTM model consists of multiple LSTM layers placed sequentially, allowing the network to learn complex temporal dependencies and hierarchical features by capturing short-term patterns in lower layers and longer-term ones in higher layers. To improve generalization and reduce overfitting, dropout is applied after each LSTM layer, randomly dropping neurons during training to prevent reliance on any single unit, which is important for deep models with many parameters. Additionally, L2 regularization adds a penalty on the squared magnitude of weights during training, encouraging smaller weights and controlling model complexity, making the combination of stacked LSTM, dropout, and L2 regularization a practical approach to building deep, robust, and expressive models for time-series forecasting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a8d4ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\win10\\python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\lstm.py:148: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 64)            33536     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66625 (260.25 KB)\n",
      "Trainable params: 66625 (260.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model with 2 stacked LSTM layers, dropout and L2 weight regularization\n",
    "def build_stacked_lstm_model(input_shape, lstm_units=64, dropout_rate=0.3, l2_reg=1e-4):\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, activation='tanh', return_sequences=True,\n",
    "             kernel_regularizer=regularizers.l2(l2_reg),\n",
    "             input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units, activation='tanh', return_sequences=False,\n",
    "             kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model_stacked = build_stacked_lstm_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "model_stacked.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "model_stacked.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6721e",
   "metadata": {},
   "source": [
    "### 2. Bidirectional LSTM Model\n",
    "\n",
    "A bidirectional LSTM (BiLSTM) enhances the traditional LSTM by processing input sequences in both forward (past to future) and backward (future to past) directions, enabling the model to access context from both preceding and succeeding time steps. This dual approach provides richer sequence understanding and captures dependencies that unidirectional LSTMs might miss, making BiLSTMs especially valuable in tasks like Remaining Useful Life (RUL) prediction and natural language processing. The outputs from both directions are combined—usually by concatenation or summation—to form a comprehensive representation at each time step. While bidirectional LSTMs improve context awareness and accuracy, they require the entire sequence upfront, leading to increased training time and computational complexity, thus being better suited for offline or batch processing rather than real-time streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db1dfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM layer to capture forward and backward temporal dependencies\n",
    "def build_bidirectional_lstm_model(input_shape, lstm_units=64, dropout_rate=0.3):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=False),\n",
    "                      input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dee442",
   "metadata": {},
   "source": [
    "### 3. Attention Mechanism Layer (Basic Additive Attention)\n",
    "\n",
    "The attention mechanism enables neural networks to dynamically focus on the most relevant parts of input sequences by computing alignment scores between the current state and each sequence element using learned weights and biases. These scores are normalized via softmax to generate attention weights that highlight important time steps, producing a weighted sum that emphasizes critical temporal information instead of treating all inputs equally. This approach overcomes the limitations of fixed-length summaries in standard RNNs or LSTMs by selectively pooling important features, improving model capacity and accuracy in tasks like language translation and time-series forecasting. Implemented as an attention layer over LSTM hidden states, it assigns importance scores that help the model focus on key sequence parts indicative of outcomes like remaining useful life, while also adding interpretability by allowing visualization of which time frames influenced predictions—making attention highly useful in predictive maintenance and RUL estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f12850b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 66)]          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 30, 64)            33536     \n",
      "                                                                 \n",
      " attention (Attention)       (None, 64)                94        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33695 (131.62 KB)\n",
      "Trainable params: 33695 (131.62 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Simple Attention Layer Definition\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# Model with attention after LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_lstm_attention_model(input_shape, lstm_units=64, dropout_rate=0.3):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    attention_out = Attention()(lstm_out)\n",
    "    dropout_out = Dropout(dropout_rate)(attention_out)\n",
    "    outputs = Dense(1)(dropout_out)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model_attention = build_lstm_attention_model(input_shape=(X.shape[1], X.shape[2]))\n",
    "model_attention.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "model_attention.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
